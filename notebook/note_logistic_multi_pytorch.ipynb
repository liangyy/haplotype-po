{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "Here I draft the core function for logistic regression using `pytorch` which solves logistic regression problem for multiple SNPs (typical use case in GWAS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "I first implement a naive convergence checker. \n",
    "And will modify the code later so that it checks and updates in per-SNP manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "# import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchIRLS(X, y, C, tol=1e-8, maxiter=100):\n",
    "    '''\n",
    "    Input\n",
    "        X: Tensor(n, p)\n",
    "        y: Tensor(n, 1)\n",
    "        C: Tensor(n, k)\n",
    "    Output:\n",
    "        B_hat: Tensor(p, 1). Effect size estimates where mth entry is for logit(y) ~ X[:, p] + C.\n",
    "        B_se: Tensor(p, 1). The corresponding SE's of the estimates.\n",
    "    '''\n",
    "    # get dimensions\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    k = C.shape[1]\n",
    "    \n",
    "    # initialize\n",
    "    Wc = torch.zeros(p, k)\n",
    "    Wx = torch.zeros(p)\n",
    "    Wcx = torch.Tensor(p, k + 1)\n",
    "    Wcx[:, :k] = Wc\n",
    "    Wcx[:, k] = Wx\n",
    "    XSX = torch.Tensor(k + 1, k + 1, p)\n",
    "    CX_RES = torch.Tensor(k + 1, p)\n",
    "    \n",
    "    diff = tol + 1\n",
    "    niter = 0\n",
    "    \n",
    "    while diff > tol and niter < maxiter:\n",
    "        \n",
    "        # take a copy of current Wcx\n",
    "        Wcx_old = Wcx.clone()\n",
    "        \n",
    "        # compute w^T x\n",
    "        C_Wc = torch.einsum('nk,pk->np', C, Wc) \n",
    "        X_Wx = torch.einsum('np,p->np', X, Wx)  \n",
    "        CX_Wcx = C_Wc + X_Wx \n",
    "        \n",
    "        # compute mu and S\n",
    "        Mu = logistic_func(CX_Wcx)\n",
    "        S = Mu * (1 - Mu)  \n",
    "        \n",
    "        # compute X^T S X\n",
    "        C_S_C = torch.einsum('nk,np,nj->kjp', C, S, C) \n",
    "        C_S_X = torch.einsum('nk,np,np->kp', C, S, X)  # .view(k, -1, p)  \n",
    "        X_S_X = torch.einsum('np,np,np->p', X, S, X)  # .view(1, 1, p)  \n",
    "        # combine blocks together\n",
    "        XSX[:k, :k, :] = C_S_C\n",
    "        XSX[:k, k, :] = C_S_X\n",
    "        XSX[k, :k, :] = C_S_X\n",
    "        XSX[k, k, :] = X_S_X\n",
    "        \n",
    "        # get LHS\n",
    "        RES = mat_vec_add(-Mu, Y)  \n",
    "        C_RES = torch.einsum('nk,np->kp', C, RES)  \n",
    "        X_RES = torch.einsum('np,np->p', X, RES)  #.view(-1, p)  \n",
    "        CX_RES[:k, :] = C_RES\n",
    "        CX_RES[k, :] = X_RES\n",
    "        \n",
    "        XSX_Wcx = torch.einsum('jkp,pk->jp', XSX, Wcx)  \n",
    "    \n",
    "        LHS = XSX_Wcx + CX_RES  \n",
    "\n",
    "        # solve and update\n",
    "        Wcx, _ = torch.solve(\n",
    "            torch.einsum('kp->pk', LHS).view(p, k + 1, -1), \n",
    "            torch.einsum('ijk->kij', XSX)\n",
    "        )\n",
    "        Wcx = Wcx[:, :, 0]\n",
    "        Wc = Wcx[:, :k]\n",
    "        Wx = Wcx[:, k]\n",
    "        \n",
    "        diff = naive_convergence_checker(Wcx, Wcx_old)\n",
    "        niter += 1\n",
    "        \n",
    "    # compute SE\n",
    "    ## need to update X^T S X first\n",
    "    # compute w^T x\n",
    "    C_Wc = torch.einsum('nk,pk->np', C, Wc) \n",
    "    X_Wx = torch.einsum('np,p->np', X, Wx)  \n",
    "    CX_Wcx = C_Wc + X_Wx \n",
    "    # compute mu and S\n",
    "    Mu = logistic_func(CX_Wcx)\n",
    "    S = Mu * (1 - Mu) \n",
    "    # compute X^T S X\n",
    "    C_S_C = torch.einsum('nk,np,nj->kjp', C, S, C) \n",
    "    C_S_X = torch.einsum('nk,np,np->kp', C, S, X)  # .view(k, -1, p)  \n",
    "    X_S_X = torch.einsum('np,np,np->p', X, S, X)  # .view(1, 1, p)  \n",
    "    ## combine blocks together\n",
    "    XSX[:k, :k, :] = C_S_C\n",
    "    XSX[:k, k, :] = C_S_X\n",
    "    XSX[k, :k, :] = C_S_X\n",
    "    XSX[k, k, :] = X_S_X\n",
    "    ## finished updating XSX\n",
    "    ONES = torch.eye(k + 1).view(k + 1, k + 1, -1).expand_as(XSX)  # Tensor(k + 1, k + 1)\n",
    "    VAR, _ = torch.solve(\n",
    "        torch.einsum('ijk->kij', ONES), \n",
    "        torch.einsum('ijk->kij', XSX)\n",
    "    )  # Tensor(k + 1, k + 1, p)\n",
    "    \n",
    "    # return B_hat, B_se\n",
    "    if diff > tol:\n",
    "        print(f'Warning: not converged! diff = {diff} > tol = {tol}')\n",
    "    return Wcx.T, torch.sqrt(torch.diagonal(VAR, dim1=1, dim2=2)).T \n",
    "\n",
    "def naive_convergence_checker(w_new, w_old):\n",
    "    nom = torch.pow(w_new - w_old, 2)\n",
    "    den = torch.pow(w_new, 2)\n",
    "    return torch.sum(nom) / torch.sum(den)\n",
    "\n",
    "def logistic_func(u):\n",
    "    return 1 / ( 1 + torch.exp(-u) )\n",
    "\n",
    "def mat_vec_add(mat, vec):\n",
    "    nrow = vec.shape[0]\n",
    "    return mat + vec.view(nrow, 1).expand_as(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate data\n",
    "\n",
    "Follow the same procedure in `../analysis/logistic_solver_single.Rmd` with the same paramter settings. \n",
    "But here we simulate multiple SNPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsnp = 10\n",
    "N = 1000\n",
    "K = 13\n",
    "maf = 0.1\n",
    "sigma_c2 = 1\n",
    "sigma2 = 4\n",
    "intercept = -3\n",
    "\n",
    "# G = torch.Tensor(N, nsnp)\n",
    "# Y = torch.Tensor(N)\n",
    "# C = torch.Tensor(N, K)\n",
    "\n",
    "# simulate data begins here\n",
    "# index of the place to fill in true signal\n",
    "idx_true = 1\n",
    "# effect sizes\n",
    "beta = torch.empty(K + 1).normal_(mean=0, std=np.sqrt(sigma2))  # np.random.normal(loc=0, scale=np.sqrt(sigma2), size=(K + 1,))\n",
    "G = torch.empty(N, nsnp).bernoulli_(maf)  # np.random.binomial(1, maf, size=(N, nsnp))\n",
    "covar = torch.empty(N, K).normal_(mean=0, std=np.sqrt(sigma_c2))  # np.random.normal(loc=0, scale=np.sqrt(sigma_c2), size=(N, K))\n",
    "X = torch.cat((torch.ones((N, 1)), G[:, idx_true, np.newaxis], covar), axis=1)\n",
    "w = torch.cat((torch.Tensor([intercept]), beta))\n",
    "mu = logistic_func(np.matmul(X, w))\n",
    "Y = torch.empty(N).bernoulli_(mu)  # rbinom(N, 1, prob = mu)\n",
    "C_w_inter = torch.cat((torch.ones((N, 1)), covar), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhat, b_se = batchIRLS(G, Y, C_w_inter, tol=1e-8)  # tol=1e-8 ~~ epsilon=1e-8 in glm.control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check (compare with R `glm`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanyul/miniconda3/lib/python3.7/site-packages/rpy2/robjects/pandas2ri.py:14: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
      "  from pandas.core.index import Index as PandasIndex\n",
      "/Users/yanyul/miniconda3/lib/python3.7/site-packages/rpy2/robjects/pandas2ri.py:34: UserWarning: pandas >= 1.0 is not supported.\n",
      "  warnings.warn('pandas >= 1.0 is not supported.')\n"
     ]
    }
   ],
   "source": [
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "stats = importr(\"stats\")\n",
    "base = importr(\"base\")\n",
    "import pandas as pd\n",
    "pandas2ri.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_glm(X, y, C):\n",
    "    Nx = X.shape[1]\n",
    "    Nc = C.shape[1]\n",
    "    bhat = np.empty((Nc + 1 + 1, Nx))\n",
    "    bse = np.empty((Nc + 1 + 1, Nx))\n",
    "    covar_df = pd.DataFrame({ f'covar{i}': C[:, i] for i in range(C.shape[1]) })\n",
    "    for xi in range(Nx):\n",
    "        string = 'cbind(y, 1 - y) ~ 1 + ' + ' + '.join(covar_df.columns.tolist()) + ' + xx'\n",
    "        data_df = pd.concat((\n",
    "            covar_df, \n",
    "            pd.DataFrame({'xx': X[:, xi]}),\n",
    "            pd.DataFrame({'y': y})\n",
    "        ), axis=1)\n",
    "        fit = stats.glm(formula=string, data=pandas2ri.py2rpy(data_df), family='binomial')\n",
    "        est = base.summary(fit).rx2('coefficients')[:, :2]\n",
    "        bhat[:, xi] = est[:, 0]\n",
    "        bse[:, xi] = est[:, 1]\n",
    "    return bhat, bse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhat0, b_se0 = logistic_regression_glm(G, Y, covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(bhat, bhat0, decimal=5)\n",
    "np.testing.assert_array_almost_equal(b_se, b_se0, decimal=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.8 ms ± 746 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "486 ms ± 16.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit batchIRLS(G, Y, C_w_inter, tol=1e-8)\n",
    "%timeit logistic_regression_glm(G, Y, covar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The `batchIRLS` implementation is consistent with `glm` upto 4 decimal but runs faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the masking scheme\n",
    "\n",
    "As first attempt, we apply mask inside the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchIRLS_w_mask(X, y, C, tol=1e-8, maxiter=100):\n",
    "    '''\n",
    "    Input\n",
    "        X: Tensor(n, p)\n",
    "        y: Tensor(n, 1)\n",
    "        C: Tensor(n, k)\n",
    "    Output:\n",
    "        B_hat: Tensor(p, 1). Effect size estimates where mth entry is for logit(y) ~ X[:, p] + C.\n",
    "        B_se: Tensor(p, 1). The corresponding SE's of the estimates.\n",
    "    '''\n",
    "    if tol > 1 or tol < 0:\n",
    "        raise ValueError('The tol is relative tolerence so we expect tol in (0, 1).')\n",
    "    # get dimensions\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    k = C.shape[1]\n",
    "    \n",
    "    # initialize\n",
    "    Wc = torch.zeros(p, k)\n",
    "    Wx = torch.zeros(p)\n",
    "    Wcx = torch.Tensor(p, k + 1)\n",
    "    Wcx[:, :k] = Wc\n",
    "    Wcx[:, k] = Wx\n",
    "    XSX = torch.Tensor(k + 1, k + 1, p)\n",
    "    CX_RES = torch.Tensor(k + 1, p)\n",
    "    mask = torch.zeros(p) == 0\n",
    "    \n",
    "    max_diff = tol + 1\n",
    "    diffs = torch.ones(p) \n",
    "    niter = 0\n",
    "    \n",
    "    while max_diff > tol and niter < maxiter:\n",
    "        \n",
    "        # generate mask\n",
    "        mask = diffs > tol\n",
    "#         print('N active = ', mask.sum(), 'max_diff = ', max_diff, 'min_diff = ', diffs.min(), 'tol = ', tol)\n",
    "        \n",
    "        # take a copy of current Wcx\n",
    "        Wcx_old = Wcx.clone()\n",
    "        \n",
    "        # compute w^T x\n",
    "        C_Wc = torch.einsum('nk,pk->np', C, Wc[mask, :])\n",
    "        X_Wx = torch.einsum('np,p->np', X[:, mask], Wx[mask])  \n",
    "        CX_Wcx = C_Wc + X_Wx \n",
    "        \n",
    "        # compute mu and S\n",
    "        Mu = logistic_func(CX_Wcx)\n",
    "        S = Mu * (1 - Mu)  \n",
    "        \n",
    "        # compute X^T S X\n",
    "        # NOTE: view is dangerous. Needs more cares!\n",
    "        C_S_C = torch.einsum('nk,np,nj->kjp', C, S, C) \n",
    "        C_S_X = torch.einsum('nk,np,np->kp', C, S, X[:, mask])  # .view(k, -1, p)  \n",
    "        X_S_X = torch.einsum('np,np,np->p', X[:, mask], S, X[:, mask])  # .view(1, 1, p)  \n",
    "        # combine blocks together\n",
    "        XSX[:k, :k, mask] = C_S_C\n",
    "        XSX[:k, k, mask] = C_S_X\n",
    "        XSX[k, :k, mask] = C_S_X\n",
    "        XSX[k, k, mask] = X_S_X\n",
    "        \n",
    "        # get LHS\n",
    "        RES = mat_vec_add(-Mu, Y)  \n",
    "        C_RES = torch.einsum('nk,np->kp', C, RES)  \n",
    "        X_RES = torch.einsum('np,np->p', X[:, mask], RES)  #.view(-1, p)  \n",
    "        CX_RES[:k, mask] = C_RES\n",
    "        CX_RES[k, mask] = X_RES\n",
    "        \n",
    "        XSX_Wcx = torch.einsum('jkp,pk->jp', XSX, Wcx)  \n",
    "    \n",
    "        LHS = XSX_Wcx + CX_RES  \n",
    "\n",
    "        # solve and update\n",
    "        tmp_, _ = torch.solve(\n",
    "            torch.einsum('kp->pk', LHS).view(p, k + 1, -1)[mask, :, :], \n",
    "            torch.einsum('ijk->kij', XSX)[mask, :, :]\n",
    "        )\n",
    "        Wcx[mask, :] = tmp_[:, :, 0]\n",
    "        Wc[mask, :] = Wcx[mask, :k]\n",
    "        Wx[mask] = Wcx[mask, k]\n",
    "        \n",
    "        max_diff, diffs = snp_level_convergence_checker(Wcx, Wcx_old)\n",
    "        niter += 1\n",
    "        \n",
    "    # compute SE\n",
    "    ## compute w^T x\n",
    "    C_Wc = torch.einsum('nk,pk->np', C, Wc)\n",
    "    X_Wx = torch.einsum('np,p->np', X, Wx)  \n",
    "    CX_Wcx = C_Wc + X_Wx \n",
    "    ## compute mu and S\n",
    "    Mu = logistic_func(CX_Wcx)\n",
    "    S = Mu * (1 - Mu)  \n",
    "    ## need to update X^T S X first\n",
    "    C_S_C = torch.einsum('nk,np,nj->kjp', C, S, C) \n",
    "    C_S_X = torch.einsum('nk,np,np->kp', C, S, X)  # .view(k, -1, p)  \n",
    "    X_S_X = torch.einsum('np,np,np->p', X, S, X)  # .view(1, 1, p)  \n",
    "    ## combine blocks together\n",
    "    XSX[:k, :k, :] = C_S_C\n",
    "    XSX[:k, k, :] = C_S_X\n",
    "    XSX[k, :k, :] = C_S_X\n",
    "    XSX[k, k, :] = X_S_X\n",
    "    ## finished updating XSX\n",
    "    ONES = torch.eye(k + 1).view(k + 1, k + 1, -1).expand_as(XSX)  # Tensor(k + 1, k + 1)\n",
    "    VAR, _ = torch.solve(\n",
    "        torch.einsum('ijk->kij', ONES), \n",
    "        torch.einsum('ijk->kij', XSX)\n",
    "    )  # Tensor(k + 1, k + 1, p)\n",
    "    \n",
    "    # return B_hat, B_se\n",
    "    if max_diff > tol:\n",
    "        print(f'Warning: not converged! max_diff = {max_diff} > tol = {tol}')\n",
    "    return Wcx.T, torch.sqrt(torch.diagonal(VAR, dim1=1, dim2=2)).T \n",
    "\n",
    "def divide_fill_zero(a, b):\n",
    "    o = torch.div(a, b)\n",
    "    o[b == 0] = 0\n",
    "    return o\n",
    "\n",
    "def snp_level_convergence_checker(w_new, w_old):\n",
    "    nom = torch.pow(w_new - w_old, 2)\n",
    "    den = torch.pow(w_new, 2)\n",
    "    diff = divide_fill_zero(torch.sum(nom, axis=1), torch.sum(den, axis=1))\n",
    "    return diff.max(), diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.4 ms ± 922 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bhat, b_se = batchIRLS(G, Y, C_w_inter, tol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.3 ms ± 1.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bhat1, b_se1 = batchIRLS_w_mask(G, Y, C_w_inter, tol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhat1, b_se1 = batchIRLS_w_mask(G, Y, C_w_inter, tol=1e-8)\n",
    "bhat, b_se = batchIRLS(G, Y, C_w_inter, tol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(bhat1, bhat0, decimal=5)\n",
    "np.testing.assert_array_almost_equal(b_se1, b_se0, decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(bhat1, bhat, decimal=5)\n",
    "np.testing.assert_array_almost_equal(b_se1, b_se, decimal=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
